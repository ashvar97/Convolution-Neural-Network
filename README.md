# Convolutional Neural Network (CNN)

A modular implementation of Convolutional Neural Networks from scratch, supporting custom architectures for image classification tasks.



## Features
- **Customizable CNN Architecture**: Modify layers, activation functions, and hyperparameters.
- **Pretrained Models Support**: Includes support for **ResNet, VGG, and MobileNet**.
- **Data Augmentation**: Implements **random cropping, flipping, rotation, and normalization**.
- **GPU Acceleration**: Optimized for execution on **CUDA-enabled GPUs**.
- **Visualization Tools**: Supports **training progress monitoring with Matplotlib and TensorBoard**.

## âœ¨ Key Features

- **Core CNN Components**:
  - 2D Convolutional layers (`Conv.py`)
  - Pooling layers (`Pooling.py`)
  - Activation functions (`ReLU.py`, `SoftMax.py`)
  
- **Training Infrastructure**:
  - Multiple optimizers (`Optimizers.py`)
  - Custom loss functions (`Loss.py`)
  - Weight initializers (`Initializers.py`)

- **Flexible Architecture**:
  - Stackable layers via `NeuralNetwork.py`
  - Configurable hyperparameters

## ðŸš€ Quick Start

1. Clone the repository:
   ```bash
   git clone https://github.com/ashvar97/Convolution-Neural-Network.git
   cd Convolution-Neural-Network

## Repository Structure
## ðŸ“‚ Repository Structure

â”œâ”€â”€ Conv.py              # 2D Convolutional layer implementation (supports padding/striding)  
â”œâ”€â”€ Flatten.py           # Flatten layer for CNN-to-FC transition (preserves batch dimension)  
â”œâ”€â”€ FullyConnected.py    # Dense/fully connected layer (configurable input/output sizes)  
â”œâ”€â”€ Initializers.py      # Weight initialization schemes (He, Xavier, normal, uniform)  
â”œâ”€â”€ Loss.py             # Loss functions: CrossEntropy (classification), MSE (regression)  
â”œâ”€â”€ NeuralNetwork.py    # Main network class (manages layer stacking/forward/backward pass)  
â”œâ”€â”€ Optimizers.py       # Gradient optimizers: SGD (with momentum), Adam, RMSprop  
â”œâ”€â”€ Pooling.py          # Pooling layers: MaxPooling (most common), AveragePooling  
â”œâ”€â”€ ReLU.py            # ReLU activation implementation (in-place operations supported)  
â””â”€â”€ SoftMax.py         # Softmax activation for multi-class classification outputs  

**Author**: Ashwin Varkey  
**Contact**: [ashvar97@gmail.com](mailto:ashvar97@gmail.com) | [LinkedIn](https://www.linkedin.com/in/ashvar97/)
